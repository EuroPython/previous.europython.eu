{"pageProps":{"path":"/session/data-validation-for-data-science","session":{"code":"SMTMWT","title":"Data Validation for Data Science","speakers":[{"code":"3Y9QDE","name":"Natan Mish","biography":"Senior Machine Learning Engineer at Zimmer Biomet - the world's leading Orthopaedic medical devices company. London School of Economics graduate with an MSc in Applied Social Data Science. Passionate about using Machine Learning to solve complicated problems. I have experience analysing, researching and building data products in the financial, real estate, transportation and healthcare industries. Curious about (almost) everything and always happy to take on new experiences and challenges.","avatar":"https://program.europython.eu/media/avatars/Screenshot_2022-03-27_at_14.28.28_jeQZNkR.png","slug":"natan-mish","affiliation":"Zimmer Biomet","homepage":"https://github.com/NatanMish","twitter":null,"biographySource":{"compiledSource":"var d=Object.defineProperty,u=Object.defineProperties;var h=Object.getOwnPropertyDescriptors;var t=Object.getOwnPropertySymbols;var r=Object.prototype.hasOwnProperty,c=Object.prototype.propertyIsEnumerable;var s=(e,n,a)=>n in e?d(e,n,{enumerable:!0,configurable:!0,writable:!0,value:a}):e[n]=a,i=(e,n)=>{for(var a in n||(n={}))r.call(n,a)&&s(e,a,n[a]);if(t)for(var a of t(n))c.call(n,a)&&s(e,a,n[a]);return e},p=(e,n)=>u(e,h(n));var l=(e,n)=>{var a={};for(var o in e)r.call(e,o)&&n.indexOf(o)<0&&(a[o]=e[o]);if(e!=null&&t)for(var o of t(e))n.indexOf(o)<0&&c.call(e,o)&&(a[o]=e[o]);return a};const layoutProps={},MDXLayout=\"wrapper\";function MDXContent(a){var o=a,{components:e}=o,n=l(o,[\"components\"]);return mdx(MDXLayout,p(i(i({},layoutProps),n),{components:e,mdxType:\"MDXLayout\"}),mdx(\"p\",null,\"Senior Machine Learning Engineer at Zimmer Biomet - the world's leading Orthopaedic medical devices company. London School of Economics graduate with an MSc in Applied Social Data Science. Passionate about using Machine Learning to solve complicated problems. I have experience analysing, researching and building data products in the financial, real estate, transportation and healthcare industries. Curious about (almost) everything and always happy to take on new experiences and challenges.\"))}MDXContent.isMDXComponent=!0;\n","scope":{}}}],"submission_type":"Tutorial","slug":"data-validation-for-data-science","track":"PyData: Data Engineering","state":"confirmed","abstract":"Have you ever worked really hard on choosing the best algorithm, tuned the parameters to perfection, and built awesome feature engineering methods only to have everything break because of a null value? Then this tutorial is for you! \r\nData validation is often neglected in the process of working on data science projects. In this tutorial, we will demonstrate the importance of implementing data validation for data science in commercial, open-source, and even hobby projects. We will then dive into some of the open-source tools available for validating data in Python and learn how to use them so that edge cases will never break our models.\r\nThe open-source Python community will come to our help and we will explore wonderful packages such as Pydantic for defining data models, Pandera for complementing the use of Pandas, and Great Expectations for diving deep into the data.\r\nThis tutorial will benefit anyone working on data projects in Python who want to learn about data validation. Some Python programming experience and understanding of data science are required. The examples used and the context of the discussion is around data science, but the knowledge can be implemented in any Python oriented project.","abstract_as_a_tweet":"Have you ever worked really hard on choosing the best algorithm, tuned the parameters to perfection, and built awesome feature engineering methods only to have everything break because of a null value? Then this tutorial is for you!","description":"For this tutorial you will need either a Google account for using Google Colaboratory, or a Python 3.8 and up environment with Jupyter installed. We will go through the hands-on exercises together in Jupyter notebooks. The context of the tutorial is a standard data science project with the common practice architecture of data ingestion, feature engineering, model training, model serving, etc.\r\nIn the first part of the tutorial, we will go through all of the common pitfalls where unexpected data values can impact the model performance, or even worse - break the run altogether. In light of the potential consequences, we will discuss the importance of data validation.\r\nFor the second part of the tutorial, we will dive into some of the open-sourced tools in the Python community that can help us with the validation task:\r\nPydantic - For defining data models, types, and simple checks.\r\nPandera - Used on top of Pandas Dataframes for schema validation.\r\nGreat Expectations - a framework for data testing, quality, and profiling.\r\nGitHub repository: https://github.com/NatanMish/data_validation","duration":"180","python_level":"some","domain_level":"some","delivery":"in-person","room":"Wicklow Hall 2A","start":"2022-07-12T13:45:00+01:00","end":"2022-07-12T15:15:00+01:00","talks_in_parallel":["B9WJUW","KM8Z88","M83YDN"],"talks_after":[],"next_talk_code":null,"prev_talk_code":null,"website_url":"https://ep2022.europython.eu/session/data-validation-for-data-science","type":"Tutorial","abstractSource":{"compiledSource":"var h=Object.defineProperty,u=Object.defineProperties;var p=Object.getOwnPropertyDescriptors;var a=Object.getOwnPropertySymbols;var r=Object.prototype.hasOwnProperty,s=Object.prototype.propertyIsEnumerable;var d=(e,o,t)=>o in e?h(e,o,{enumerable:!0,configurable:!0,writable:!0,value:t}):e[o]=t,i=(e,o)=>{for(var t in o||(o={}))r.call(o,t)&&d(e,t,o[t]);if(a)for(var t of a(o))s.call(o,t)&&d(e,t,o[t]);return e},l=(e,o)=>u(e,p(o));var c=(e,o)=>{var t={};for(var n in e)r.call(e,n)&&o.indexOf(n)<0&&(t[n]=e[n]);if(e!=null&&a)for(var n of a(e))o.indexOf(n)<0&&s.call(e,n)&&(t[n]=e[n]);return t};const layoutProps={},MDXLayout=\"wrapper\";function MDXContent(t){var n=t,{components:e}=n,o=c(n,[\"components\"]);return mdx(MDXLayout,l(i(i({},layoutProps),o),{components:e,mdxType:\"MDXLayout\"}),mdx(\"p\",null,`Have you ever worked really hard on choosing the best algorithm, tuned the parameters to perfection, and built awesome feature engineering methods only to have everything break because of a null value? Then this tutorial is for you!\nData validation is often neglected in the process of working on data science projects. In this tutorial, we will demonstrate the importance of implementing data validation for data science in commercial, open-source, and even hobby projects. We will then dive into some of the open-source tools available for validating data in Python and learn how to use them so that edge cases will never break our models.\nThe open-source Python community will come to our help and we will explore wonderful packages such as Pydantic for defining data models, Pandera for complementing the use of Pandas, and Great Expectations for diving deep into the data.\nThis tutorial will benefit anyone working on data projects in Python who want to learn about data validation. Some Python programming experience and understanding of data science are required. The examples used and the context of the discussion is around data science, but the knowledge can be implemented in any Python oriented project.`))}MDXContent.isMDXComponent=!0;\n","scope":{}},"descriptionSource":{"compiledSource":"var d=Object.defineProperty,p=Object.defineProperties;var u=Object.getOwnPropertyDescriptors;var n=Object.getOwnPropertySymbols;var r=Object.prototype.hasOwnProperty,s=Object.prototype.propertyIsEnumerable;var h=(t,e,o)=>e in t?d(t,e,{enumerable:!0,configurable:!0,writable:!0,value:o}):t[e]=o,i=(t,e)=>{for(var o in e||(e={}))r.call(e,o)&&h(t,o,e[o]);if(n)for(var o of n(e))s.call(e,o)&&h(t,o,e[o]);return t},l=(t,e)=>p(t,u(e));var c=(t,e)=>{var o={};for(var a in t)r.call(t,a)&&e.indexOf(a)<0&&(o[a]=t[a]);if(t!=null&&n)for(var a of n(t))e.indexOf(a)<0&&s.call(t,a)&&(o[a]=t[a]);return o};const layoutProps={},MDXLayout=\"wrapper\";function MDXContent(o){var a=o,{components:t}=a,e=c(a,[\"components\"]);return mdx(MDXLayout,l(i(i({},layoutProps),e),{components:t,mdxType:\"MDXLayout\"}),mdx(\"p\",null,`For this tutorial you will need either a Google account for using Google Colaboratory, or a Python 3.8 and up environment with Jupyter installed. We will go through the hands-on exercises together in Jupyter notebooks. The context of the tutorial is a standard data science project with the common practice architecture of data ingestion, feature engineering, model training, model serving, etc.\nIn the first part of the tutorial, we will go through all of the common pitfalls where unexpected data values can impact the model performance, or even worse - break the run altogether. In light of the potential consequences, we will discuss the importance of data validation.\nFor the second part of the tutorial, we will dive into some of the open-sourced tools in the Python community that can help us with the validation task:\nPydantic - For defining data models, types, and simple checks.\nPandera - Used on top of Pandas Dataframes for schema validation.\nGreat Expectations - a framework for data testing, quality, and profiling.\nGitHub repository: `,mdx(\"a\",i({parentName:\"p\"},{href:\"https://github.com/NatanMish/data_validation\"}),\"https://github.com/NatanMish/data_validation\")))}MDXContent.isMDXComponent=!0;\n","scope":{}}},"sessionsAfter":[],"sessionsInParallel":[{"code":"B9WJUW","title":"How to setup your development workflow to keep your code clean","speakers":[{"code":"LG8REN","name":"Guillaume Dequenne","biography":"I work for Sonar in Geneva, Switzerland where I develop static code analysis tools for Python.\r\n\r\nWhen not working, I'm usually found snowboarding or hiking in the mountains.","avatar":"https://program.europython.eu/media/avatars/gdequenne_portrait_skyuAvN.jpg","slug":"guillaume-dequenne","affiliation":"SonarSource","homepage":null,"twitter":null}],"submission_type":"Tutorial","slug":"how-to-setup-your-development-workflow-to-keep-your-code-clean","track":"Sponsor","state":"confirmed","abstract":"Clean code is something every developer should aim for, but how to make sure code is actually clean? How much should be invested in that endeavor? Whose responsibility is it?\r\n\r\nIn this workshop, we will go through all the aspects and stages to setup your development workflow to help you take ownership of the quality of your code.\r\n\r\nWe will take a simple application as a starting point and simulate a full development cycle, including coding in the IDE and opening a pull request on GitHub. We will create a CI pipeline triggering code quality monitoring using Sonar tools. More specifically, we will be using SonarCloud as a central platform to monitor code quality and SonarLint to detect issues directly in the IDE. \r\n\r\nAt the end of the workshop, you will be ready to enable such integration for your own projects.","abstract_as_a_tweet":"Clean code is something every developer should aim for, but how to make sure code is actually clean? How much should be invested in that endeavor? Whose responsibility is it?","description":"Making sure to keep a clean codebase during the lifetime of a project is no easy task. Once technical debt starts accruing, we might feel it’s pointless to try to revert the trend.\r\n\r\nThroughout the workshop, we’ll use a small Flask application and simulate the review of a new feature and see how we can make sure to deliver clean code at each stage of the development workflow.\r\n\r\nTo follow the workshop in the best conditions, it would be good to meet the following prerequisites:\r\n\r\n* Have an account on GitHub and be logged-in\r\n* Have git and an IDE ready (preferably PyCharm or VSCode)\r\n* Optionally: have a Python virtual environment ready to run the application locally\r\n\r\nThe application lives in the following GitHub repository: https://github.com/SonarEuroPython/sonar-europython\r\n\r\nAttendees will be invited to fork this repository in their personal organization in order to follow the workshop.","duration":"180","python_level":"some","domain_level":"some","delivery":"in-person","room":"Liffey Hall 2","start":"2022-07-12T13:45:00+01:00","end":"2022-07-12T15:15:00+01:00","talks_in_parallel":["KM8Z88","M83YDN","SMTMWT"],"talks_after":[],"next_talk_code":null,"prev_talk_code":null,"website_url":"https://ep2022.europython.eu/session/how-to-setup-your-development-workflow-to-keep-your-code-clean"},{"code":"KM8Z88","title":"Build with Audio: The easy & hard way!","speakers":[{"code":"DHPEJJ","name":"Vaibhav Srivastav","biography":"I am a Data Scientist and a Masters Candidate - Computational Linguistics at Universität Stuttgart. I am currently researching on Speech, Language and Vision methods for extracting value out of unstructured data.\r\n\r\nIn my previous stint with Deloitte Consulting LLP, I worked with Fortune Technology 10 clients to help them make data-driven (profitable) decisions. In my surplus time, I served as a Subject Matter Expert on Google Cloud Platform to help build scalable, resilient and fault-tolerant cloud workflows.\r\n\r\nBefore this, I have worked with startups across India to build Social Media Analytics Dashboards, Chat-bots, Recommendation Engines, and Forecasting Models.\r\n\r\nMy core interests lie in Natural Language Processing, Machine Learning/ Statistics and Cloud based Product development.\r\n\r\nApart from work and studies, I love travelling and delivering Workshops/ Talks at conferences and events across APAC and EU, DevConf.CZ, Berlin Buzzwords, DeveloperDays Poland, PyCon APAC (Philippines), Korea, Malaysia, Singapore, India, WWCode Asia Connect, Google DevFest, and Google Cloud Summit.","avatar":"https://program.europython.eu/media/avatars/JFlrSzB4_400x400_e1UV9lu.jpg","slug":"vaibhav-srivastav","affiliation":"University of Stuttgart","homepage":null,"twitter":"@reach_vb"}],"submission_type":"Tutorial","slug":"build-with-audio-the-easy-hard-way","track":"PyData: Deep Learning, NLP, CV","state":"confirmed","abstract":"The audio (& speech) domain is going through a massive shift in terms of end-user performances. It is at the same tipping point as NLP was in 2017 before the Transformers revolution took over. We’ve gone from needing a copious amount of data to create Spoken Language Understanding systems to just needing a 10-minute snippet. \r\n\r\nThis tutorial will help you create strong code-first & scientific foundations in dealing with Audio data and build real-world applications like Automatic Speech Recognition (ASR) Audio Classification, and Speaker Verification using backbone models like Wav2Vec2.0, HuBERT, etc.","abstract_as_a_tweet":"Learn how to build, demystify and deploy State-of-The-Art audio models, all in less than 3 hours!","description":"Repository for the conference: https://github.com/Vaibhavs10/how-to-asr\r\n\r\nUnlike general Machine Learning problems where we either classify i.e. segregate a data point into a pre-defined class or regress around a continuous variable, audio related problems can be slightly more complex. Wherein, we either go from an audio representation to a text representation (ASR) or separate different layers of audio (Diarization) and so on. This tutorial will not only help you build applications like these but also unpack the science behind them using a code-first approach.\r\n\r\nEvery step of the way we’ll first write and run some code and then take a step back and unpack it all till it makes sense. We’ll make science *fun* again :)\r\n\r\nThe tutorial will be divided into 3 key sections:\r\n\r\n1. Read, Manipulate & Visualize Audio data\r\n2. Build your very own ASR system (using pre-trained models like Wav2Vec2.0) & deploy it\r\n3. Create an Audio Classification pipeline & infer the model for other downstream audio tasks \r\n\r\nAt the end of the tutorial, you’ll develop strong intuition about Audio data and learn how to leverage large pre-trained backbone models for downstream tasks. You’ll also learn how to create quick demos to test and share your models.\r\n\r\nLibraries: HuggingFace, SpeechBrain, PyTorch & Librosa","duration":"180","python_level":"some","domain_level":"some","delivery":"in-person","room":"Wicklow Hall 2B","start":"2022-07-12T13:45:00+01:00","end":"2022-07-12T15:15:00+01:00","talks_in_parallel":["B9WJUW","M83YDN","SMTMWT"],"talks_after":[],"next_talk_code":null,"prev_talk_code":null,"website_url":"https://ep2022.europython.eu/session/build-with-audio-the-easy-hard-way"},{"code":"M83YDN","title":"Norvig's lispy: beautiful and illuminating Python code","speakers":[{"code":"AX8V78","name":"Luciano Ramalho","biography":"Luciano Ramalho is the author of Fluent Python, published in 9 languages. He is a Principal Consultant at Thoughtworks and a Fellow of the Python Software Foundation.","avatar":"https://program.europython.eu/media/avatars/LucianoRamalho2016-500x.jpg","slug":"luciano-ramalho","affiliation":"Thoughtworks","homepage":"https://www.thoughtworks.com/en-br/profiles/l/luciano-ramalho","twitter":"@ramalhoorg"}],"submission_type":"Tutorial","slug":"norvig-s-lispy-beautiful-and-illuminating-python-code","track":"Python Friends","state":"confirmed","abstract":"Why isn't `if` a function? Why does Python need to add keywords from time to time? What precisely is a closure, what problem does it solve, and how does it work? These are some of the fundamental questions you'll be able to answer after this tutorial: an interactive exploration of Peter Norvig's  `lis.py`–an interpreter for a subset of the Scheme dialect of Lisp in 132 lines of Python.","abstract_as_a_tweet":"Norvig's lis.py: a Scheme interpreter in 132 lines of readable Python code. Let's see how a language works!","description":"Peter Norvig of Stanford University wrote `lis.py`: an interpreter for a subset of the Scheme dialect of Lisp in 132 lines of readable Python. I took Norvig's code, updated it to modern Python coding style, and integrated it into a Jupyter notebook that provides explanations as well as interactive experiments and exercises checked automatically.\r\n\r\nWhy should you study lis.py? This is what I got out of it:\r\n\r\n* Learning how an interpreter works gave me a deeper understanding of Python and programming languages in general—interpreted or compiled.\r\n\r\n* The simplicity of Scheme is a master class of language design.\r\n\r\n* `lis.py` is a beautiful example of idiomatic Python code.","duration":"180","python_level":"some","domain_level":"none","delivery":"in-person","room":"Wicklow Hall 1","start":"2022-07-12T13:45:00+01:00","end":"2022-07-12T15:15:00+01:00","talks_in_parallel":["B9WJUW","KM8Z88","SMTMWT"],"talks_after":[],"next_talk_code":null,"prev_talk_code":null,"website_url":"https://ep2022.europython.eu/session/norvig-s-lispy-beautiful-and-illuminating-python-code"}]},"__N_SSG":true}